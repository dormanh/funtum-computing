## Description of the problem

We would like to find the prime factors $p, q \in \mathbb{P}$ of a large {{semiprime|wiki:Semiprime}} $N \in \mathbb{N}$ (so that $N = p \cdot q$). Finding these factors for a small number, such as $15$, is quite easy. In fact, you may figure it out just by looking at it and recalling your grade school studies: the factors are $3$ and $5$. But what are the prime factors of, say, $62615533$? It turns out that, using the best known classical (meaning non-quantum) algorithm (the so called {{number field sieve|wiki:General_number_field_sieve}}), solving the factoring problem scales {{exponentially|wiki:Exponential_function}} with the number of digits of the semiprime that we want to factor. With the help of Shor's algorithm and a sufficiently large {{quantum computer|wiki:Quantum_computing}}, the same problem scales roughly {{cubically|wiki:Cubic_function}} with the number of digits, meaning that the number of elementary operations required is proportional to (the number of digits)$^3$. (In fact, the algorithm does slighly better than that, but the precise formula is a bit complicated and is beside the point.) To illustrate what this means in practical terms, imagine the following. If factoring a $100$-digit number with Shor's algorithm on a quantum computer takes one second, factoring the same number with the number field sieve on a classical computer takes more than $3$ hours. If we now try with a $200$-digit number, Shor's algorithm finishes in roughly $5$ seconds, while the classical solution would take more than a year!

{{Computational complexity|wiki:Computational_complexity}} refers to how the time - or number of elementary operations - required to solve a given problem algorithmically grows with the size of the input. Let's say you have to find the longest book you have ever read. One way to approach this problem is to list the books you have read one by one, label the first one as the longest, and whenever you encounter an even longer book, it takes the place of the former. This way, you only have to keep track of the largest page number so far, and the book associated with it. The "elementary operation" in this case is checking the number of pages of the next book and comparing that with the previous maximum. If you have twice or three times as many books, this procedure requires twice or three times as many elementary operations. In other words, it grows {{linearly|wiki:Linear_function}} in the number of books. So, while the solution can be found using the same algorithm, the number of elementary operations required to carry out that algorithm depends on the size of the input. This is true for almost all algorithms that are designed to solve mathematical problems. However, how fast that growth is, makes all the difference between them.


<!-- TODO add and link to some use cases like RSA here and explain how this is important and strange. -->